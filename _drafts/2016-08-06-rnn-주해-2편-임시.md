---
title: rnn 주해 2편
date: 2016-08-06 23:00
categories:
- 딥러닝
tags:
- RNN
- 텍스트
layout: post
published: true
---

```python
while True:
  # 입력데이터 준비, 텍스트의 맨 앞쪽부터 seq_length만큼씩 데이터를 준비
  # 데이터를 모두 사용하면 입력 데이터의 맨 처음으로 이동
  if p+seq_length+1 >= len(data) or n == 0: 
    hprev = np.zeros((hidden_size,1)) # RNN 메로리 초기화
    p = 0 # 입력 데이터의 맨 처음으로 이동
  
  # 입력(p~p+24번째 글자), 목표(p+1~p+25번째 글자) 데이터를 준비 
  inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]
  targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]
```

메인 루프의 시작 부분이다. 데이터를 읽어들일 위치를 p로 정하고, p번째 글자부터 25개의 글자를 준비한다. min-char-rnn 모델의 목적은 어떤 글자를 넣었을 때, 다음 글자를 추론해 내는 것이 목적이기 때문에, 정답 세트로 p+1번째 글자부터 25개의 글자를 준비한다.

모델에 데이터를 입력하기 위해 문자를 char_to_ix 사전을 이용하여 각 문자에 대응하는 숫자의 리스트로 변환하여 준다. 

```python
  # 학습을 100번 반복할 때마다 학습 결과를 출력
  if n % 100 == 0:
    sample_ix = sample(hprev, inputs[0], 200) #sample 함수로 
    txt = ''.join(ix_to_char[ix] for ix in sample_ix)
    print('----\n %s \n----' % (txt, ))
```

이 부분에서는 sample 함수를 이용하여 학습의 결과를 출력하는 부분이다. 200 글자를 생성하기 위한 초기값은 현재까지 학습된 hprev와 학습 데이터의 첫번째 글자를 학습 데이터로 사용한다.
해당 함수의 자세한 내용은 다음 글에서 설명할 예정이다.
 ix_to_char 사전으로 숫자들을 문자로 변환하여 학습의 결과를 출력해준다.

```python
  # 손실함수에서 손실값과 그래디언트를 함께 계산
  loss, dWxh, dWhh, dWhy, dbh, dby, hprev = lossFun(inputs, targets, hprev)
  smooth_loss = smooth_loss * 0.999 + loss * 0.001
  if n % 100 == 0: print('iter %d, loss: %f' % (n, smooth_loss)) # 반복횟수, 손실 출력
```

lossFun 함수에서 손실값 및 그래디언트를 계산하여 값을 가져온다. loss값을 출력해 줄 때에는 smooth_loss라는 값을 사용하는데, 사람이 인식할 때 loss값이 왔다갔다가 심하면 학습이 잘못되고 있는 것으로 오해할 수 있기 때문에 
회당 현재 loss값을 1/1000으로 줄이고 기존 값에 .999 가중치를 줘서 loss의 변화를 스무스하게 출력해 주는 것으로 보인다.

$$
0.001 \left (  0.999^{99}\times L_{1} + 0.999^{98}\times L_{2} + \cdots + L_{100} \right )
$$

이런 식으로 smooth_loss에 반영되는 식임. 100번마다 학습 결과를 출력해 주기 때문에 100번의 학습한 값이 반영되는 정도는 단순하게 생각해서 약 9.5% 정도 반영이 된다 보면 되겠다.

```python
  # Adagrad 방식으로 파라미터 업데이트
  for param, dparam, mem in zip([Wxh,  Whh,  Why,  bh,  by],   # 가중치
                                [dWxh, dWhh, dWhy, dbh, dby],  # 그래디언트
                                [mWxh, mWhh, mWhy, mbh, mby]): # 메모리 
    mem += dparam * dparam
    param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # 실제 파라메터 업데이트

  p += seq_length # 데이터 포인터를 seq_length만큼 우측으로 이동
  n += 1 # 반복횟수 카운터
```



## Adagrad 방식 파라메터 업데이트

min-char-rnn에서는 가중치를 업데이트하기 위한 알고리즘으로 adagrad 알고리즘을 하용하고 있다. 모든 가중치에 대해 일괄적으로 learning_rate에 의해서 업데이트 시키는 방법보다 속도가 매우 빠르다는 장점을 가지고 있다.
그래디언트에 일괄적인 학습 속도를 적용하기 보다는 행렬로 나타내었을 때 변화폭이 큰 항목에 대해서는 이동폭을 작게 조정해주고, 그래를 가진 값에 대해서는 이동폭을 크게 하고, 가중치가 큰 항목에 대해서는 이동 폭을 줄이는 알고리즘이다. 예를 들어 가중치 행렬 Wxh가 있다고 할 때 아래 행렬과 같다고 가정하면..

$$
W_{xh} = \begin{pmatrix}
3 & -0.5 & -2 \\ 
1.1 & 0.03 & \textbf{0.001} \\ 
-1.1 & \textbf{-5} & -0.14
\end{pmatrix}
$$

`param += -learning_rate * dparam / np.sqrt(mem + 1e-8)`{: .language-python} 이기 때문에 분모가 mem 이므로 mem이 커질수록 각 원소의 변화폭이 감소한다.
`mem += dparam * dparam`{: .language-python} 에 의해서 dparam의 제곱만큼을 계속 더하기만 하기 때문에 점점 변화하는 폭은 줄어들기만 하기 때문에 점차 굵게 표시된 두 숫자에 대해서만 예를 들자면... 속도가 점점 느려지게 되는 특성을 보인다. 
0.001에 대해서는 0.001 만 분모에 추가되지만, 

한번에 이동하는 그래디언트가 클수록 이동하는 속도가 점차 느려진다고 생각하면 되겠다. learning rate mem이 점점 커지기 때문에 이동하는 속도가 점점 느려지는 방향으로만 작동하기 때문에 
그냥 SGD보다 빠른 방법이라고 생각하면 된다. 각 머신러닝 패키지에서 지원하는





